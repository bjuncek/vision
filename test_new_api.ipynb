{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, av\n",
    "video_path = \"video_reader_benchmark/videos/R6llTwEh07w.mp4\"\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `next_tensor` vs `next_list`\n",
    "testing that we get the same frame using both functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "t1, _ = video.next_list(\"\")\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "t2 = video.next_tensor(\"\")\n",
    "# first we assert that these two are the same tensors\n",
    "assert torch.equal(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timing\n",
    "comparing the timeits of both (unscientific; for benchmarking check [here](https://github.com/bjuncek/video_reader_benchmark/blob/bkorbar/newAPI/timeitcomp/Graph%20Results.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.97 ms ± 4.06 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "_, _ = video.next_list(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02 ms ± 80.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "_ = video.next_tensor(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. returting the dummy tensor alone vs returining it in a list\n",
    "the two returned tensors should be equal,\n",
    "furthermore, they should be the same in size as the benchmark tensor from point 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "t3, _ = video.next_list_dummy_tensor(\"\")\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "t4 = video.next_tensor_dummy_tensor(\"\")\n",
    "\n",
    "assert torch.equal(t3, t4)\n",
    "assert t1.size() == t3.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timing\n",
    "comparing the timeits of both (unscientific; for benchmarking check [here](https://github.com/bjuncek/video_reader_benchmark/blob/bkorbar/newAPI/timeitcomp/Graph%20Results.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02 ms ± 138 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "_, _ = video.next_list_dummy_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1 ms ± 164 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "video = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "_ = video.next_tensor_dummy_tensor(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Looped decoding: tensor vs list vs num_frames\n",
    "we're checking for 2 assumptions:\n",
    "1. two tensor lists will be identical\n",
    "2. they will have the same number of frames compared to simply returning the decoded part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_1 = []\n",
    "t, _ = reader.next_list(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_1.append(t)\n",
    "    t, _ = reader.next_list(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "i = reader.next_int_numframes(\"\")\n",
    "f_3 = []\n",
    "while i == 1:\n",
    "    i = reader.next_int_numframes(\"\")\n",
    "    f_3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition 1\n",
    "assert len(f_1) == len(f_2)\n",
    "assert len(f_1) == len(f_3)\n",
    "\n",
    "# condition 2\n",
    "assert torch.equal(torch.stack(f_1, 0), torch.stack(f_2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timing\n",
    "comparing the timeits of both (unscientific; for benchmarking check [here](https://github.com/bjuncek/video_reader_benchmark/blob/bkorbar/newAPI/timeitcomp/Graph%20Results.ipynb))\n",
    "Note that all functions are decoding, the only difference should be the return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 ms ± 9.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_1 = []\n",
    "t, _ = reader.next_list(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_1.append(t)\n",
    "    t, _ = reader.next_list(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 ms ± 9.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 ms ± 4.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "i = reader.next_int_numframes(\"\")\n",
    "f_3 = []\n",
    "while i == 1:\n",
    "    i = reader.next_int_numframes(\"\")\n",
    "    f_3.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 comparing the speed of decoding a real tensor vs returing a dummy tensor\n",
    "Note that both functions are decoding, the only difference (in theory) should be that `next_tensor` is filling the tensor with actual values, while\n",
    "`next_tensor_dummy_tensor` is filling it with ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 303\n"
     ]
    }
   ],
   "source": [
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor(\"\")\n",
    "\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_3 = []\n",
    "t = reader.next_tensor_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_3.append(t)\n",
    "    t = reader.next_tensor_dummy_tensor(\"\")\n",
    "\n",
    "assert len(f_2) == len(f_3)\n",
    "print(len(f_2), len(f_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 ms ± 1.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 ms ± 7.08 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_3 = []\n",
    "t = reader.next_tensor_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_3.append(t)\n",
    "    t = reader.next_tensor_dummy_tensor(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, the most interesting test,\n",
    "## 3.3 comparing the difference between returing a dummy tensor and a dummy list\n",
    "\n",
    "Note that both functions are decoding, the only difference (in theory) should be that `next_list_dummy_tensor` is returning the list with dummy tensors, and\n",
    "`next_tensor_dummy_tensor` is filling it with ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 303\n"
     ]
    }
   ],
   "source": [
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor_dummy_tensor(\"\")\n",
    "    \n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_3 = []\n",
    "t, _ = reader.next_list_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_3.append(t)\n",
    "    t, _ = reader.next_list_dummy_tensor(\"\")\n",
    "\n",
    "print(len(f_2), len(f_3))\n",
    "assert len(f_2) == len(f_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 ms ± 5.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t = reader.next_tensor_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t = reader.next_tensor_dummy_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 ms ± 2.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "reader = torch.classes.torchvision.Video(video_path, \"video\", True)\n",
    "f_2 = []\n",
    "t, _ = reader.next_list_dummy_tensor(\"\")\n",
    "while t.numel() > 0:\n",
    "    f_2.append(t)\n",
    "    t, _ = reader.next_list_dummy_tensor(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
